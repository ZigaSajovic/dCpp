{
  "name": "Dcpp",
  "tagline": "Differentiable C++; conditionals, loops, recursion and all things C++",
  "body": "# dCpp\r\nInfinitely differentiable C++; conditionals, loops, recursion and all things C++\r\n\r\n###Abstract\r\nWe provide an illustrative implementation of an analytic, infinitely-differentiable machine, implementing infinitely-differentiable programming spaces and operators acting upon them, as constructed in the paper _Operational calculus on programming spaces and generalized tensor networks_. Implementation closely follows theorems and derivations of the paper, intended as an educational guide.\r\n\r\nThis is the openSource version.\r\n\r\n###Theory\r\n\r\nFrom _abstract_ of the paper  _Operational calculus on programming spaces and generalized tensor networks_ in which I derived the theory\r\n\r\nIn this paper, we develop the theory of analytic virtual machines, that\r\nimplement analytic programming spaces and operators acting upon them. Such a machine fully integrates control structures, reataining the expressive freedom of algorithmic control flow.\r\n\r\nA programming space is a subspace of the function space of maps on the virtual\r\nmemory. We can construct a differential operator on programming spaces as we \r\nextend the virtual memory to a tensor product of a virtual space with tensor algebra\r\nof its dual. Extended virtual memory serves by itself as an algebra of programs, giving the expansion of the original program as an infinite tensor series at\r\nprogram's input values. \r\n\r\nA paper explaining implementation of this theory is avaliable [/paper/dCpp.pdf](https://zigasajovic.github.io/dCpp/paper/dCpp.pdf).\r\n\r\nPaper with construction of the theory will soon be available on arXiv.\r\n\r\n###Usage\r\nBy employing analytic virtual machines, we can construct analytic procedures, viewing algorithms in a new light. One can start incorporating variable parameters into algorithm design, revealing the true nature of hyper-parameters often used in practice.\r\n\r\n###Tutorial\r\nAs most programmers face the need of differentiability through machine learning, we use the concept a [Recurrent neural network](https://en.wikipedia.org/wiki/Recurrent_neural_network) employing [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) with [softmax normalization](https://en.wikipedia.org/wiki/Softmax_function#Softmax_Normalization) as a vessel for this tutorial. We demostrate, how it is simply constructed using algorithmic control flow and reccursion.\r\n\r\nFirst we include the necessities\r\n\r\n```c++\r\n#include <iostream>\r\n#include <dCpp.h>\r\n#include <vector>\r\n```\r\n\r\nWe initialize a n-differentiable programming space (order is arbitrary here)\r\n\r\n```c++\r\nusing namespace dCpp;\r\nint n_differentiable=2;\r\ninitSpace(n_differentiable);\r\n```\r\n\r\nWe will need the folowing functions\r\n* [sigmoid(x)](https://en.wikipedia.org/wiki/Sigmoid_function)\r\n* [softmax(vec)](https://en.wikipedia.org/wiki/Softmax_function)\r\n* [dotProduct(vec1,vec2)](https://en.wikipedia.org/wiki/Dot_product)\r\n* [matVecProduct(mat,vec)](https://en.wikipedia.org/wiki/Matrix_multiplication)\r\n\r\nBy coding sigmoid(x), we will learn about creating differentiable maps, constructable using the differentiable programming space _dCpp_ and the algebra of the virtual memory _var_.\r\nFirst we create maps double->double, for e(x) and its' derivative.\r\n```c++\r\nvar sigmoidMap(const var&v){return 1/(1+exp(-1*v));};\r\n\r\n```\r\n\r\nWe test it out and display all first and second derivatives.\r\n\r\n```c++\r\n//  set inputs\r\n    double x=4;\r\n    double y=2;\r\n//  set weights\r\n    var w_1(0.4);\r\n    var w_2(0.6);\r\n//  initialize weights as twice differentiable variables\r\n    dCpp::init(w_1);\r\n    dCpp::init(w_2);\r\n//  now we use sigmoid map as a differentiable map\r\n    var f=sigmoidMap(w_1*x+w_2*y);\r\n//  df/dx\r\n    std::cout<<\"df/dw_1 = \"<<f.d(&w_1).id<<std::endl;\r\n//  df/dw_2\r\n    std::cout<<\"df/dw_2 = \"<<f.d(&w_2).id<<std::endl;\r\n//  df/dw_1dw_1\r\n    std::cout<<\"df/dw_1dw_1 = \"<<f.d(&w_1).d(&w_1).id<<std::endl;\r\n//  df/dw_1dw_2\r\n    std::cout<<\"df/dw_1dw_2 = \"<<f.d(&w_1).d(&w_2).id<<std::endl;\r\n//  df/dw_2dw_1\r\n    std::cout<<\"df/dw_2dw_1 = \"<<f.d(&w_2).d(&w_1).id<<std::endl;\r\n//  df/dw_2dw_2\r\n    std::cout<<\"df/dw_2dw_2 = \"<<f.d(&w_2).d(&w_2).id<<std::endl;\r\n```\r\n\r\n Similarly, we could have used the operator [tau](include/tau.h) by coding , which allows one to create it's own elements of the differentiable programming space _dCpp_, returning a differentiable variable [var](/include/var.h).\r\n```\r\nBy coding the softmax normalization, we reveal how analytic differentiable machines fully integrate control structures.\r\n```c++\r\n//simply code the map existing in the programming space dCpp\r\n//and the belonging algebra\r\nstd::vector<var> softmax(const std::vector<var>& V){\r\n    std::vector<var> out;\r\n    var sum(0);\r\n    init(sum);\r\n    for(var v:V){\r\n        sum=sum+exp(v);\r\n    }\r\n    for(var v:V){\r\n        out.push_back(exp(v)/sum);\r\n    }\r\n    return out;\r\n}\r\n\r\n```\r\nWe test it, by inititalizing a four-differentiable programming space and displaying all derivatives.\r\n\r\n```c++\r\n//  initiaize Virtual memory of fourth order\r\n    initSpace(4);\r\n//get a vector of variables\r\n    int size=2;\r\n    std::vector<var> vars;\r\n    for(int i=1;i<=size;i++){\r\n        var tmp=var(i);\r\n        init(tmp);\r\n        vars.push_back(tmp);\r\n    }\r\n//  use the softmax function\r\n    std::vector<var> f=softmax(vars);\r\n//  display derivatives of all four orders\r\n//   of one of the components\r\n    f[1].print();\r\n\r\n```\r\n\r\nAssume existence of functions _vecSum_, _matVecProd_, _genRandVec_ and _forAll_ written in a similar fashion. Thus, we have all the tools needed to build a recursive layer. It will consist of two layers, mapping a 2-vector to a 2-vector. Output of the second layer will be recursively connected to the input of the next recursive layer.\r\n\r\nFor brevity, we denote _std::vector<std::vector<var> >_ by _mat_ and _std::vector<var>_ by _vec_.\r\n\r\n```c++\r\nvec recursionNet(vec input, mat weights[2],vec bias[], int depth){\r\n    if(depth==0){\r\n        return softmax(input);\r\n    }\r\n    else{\r\n        vec firstOut;\r\n        //matrix vector multiplication\r\n        firstOut=matVecProd(weights[0],input);\r\n        firstOut=vecSum(firstOut,bias[0]);\r\n        forAll(firstOut,sigmoid);\r\n        vec secondOut;\r\n        secondOut=matVecProd(weights[1],firstOut);\r\n        secondOut=vecSum(secondOut,bias[1]);\r\n        forAll(secondOut,sigmoid);\r\n        return recursionNet(secondOut,weights, bias,depth-1);\r\n    }\r\n}\r\n```\r\n\r\nNow only some initialization of weights is needed and the network can be used, exactly like any other function would, with the exception, that this function is _n-differentiable_.\r\n\r\n```c++\r\nvec output = recursionNet(input,weights[],bias[], depth);\r\nfor(var v:output)v.print();\r\n```\r\n\r\nto display derivatives of all orders, upt to _n_ by which the space has been initialized.\r\n\r\n###External libraries\r\n\r\nUsage with external libraries written in generic paradigm is demonstrated on the example of [Eigen](http://eigen.tuxfamily.org/). \r\nWe will code a perceptron with sigmoid activations, followed by softmax normalization, taking 28x28 image as an input and outputting a 10 class classifier. We will use dCpp provided mappings in the _dEigen_ header.\r\n\r\nThis perceptron is _4-differentiable_.\r\n\r\n```c++\r\n#include <iostream>\r\n#include <dCpp.h>\r\n#include <dEigen.h>\r\nusing namespace std;\r\nusing namespace dC;\r\n\r\n//create a softmax function\r\ntemplate <typename Derived>\r\n    void softmax(Eigen::MatrixBase<Derived>& matrix){\r\n            //maps each element of the matrix by y=e^x;\r\n            dC::map_by_element(matrix,&dC::exp);\r\n            //sums the elements of the matrix using Eigens function\r\n            var tmp=matrix.sum();\r\n            //divides each element by the sum\r\n            for (size_t i=0, nRows=matrix.rows(), nCols=matrix.cols(); i<nCols; ++i)\r\n                for (size_t j=0; j<nRows; ++j){\r\n                    matrix(j,i)/=tmp;\r\n                }\r\n}\r\n\r\nint main(){\r\n    initSpace(4);\r\n    //    Matrix holding the inputs (imgSizeX1 vector)\r\n    const int imgSize=28*28;\r\n    const Eigen::Matrix<double,1,imgSize>input=Eigen::Matrix<var,1,imgSize>::Random(1,imgSize);\r\n    //    number of outputs of the layer\r\n    const int numOfOutOnFirstLevel=10;\r\n    //    matrix of weights on the first level (imgSizeXnumOfOutOnFirstLevel)\r\n    Eigen::Matrix<var,imgSize,numOfOutOnFirstLevel>firstLayerVars=Eigen::Matrix<var,imgSize,numOfOutOnFirstLevel>::Random(imgSize,numOfOutOnFirstLevel);\r\n    //    initializing weights\r\n    dC::init(firstLayerVars);\r\n    //    mapping of the first layer --> resulting in 10x1 vector\r\n    Eigen::Matrix<var,numOfOutOnFirstLevel,1>firstLayerOutput=input*firstLayerVars;\r\n    //    apply sigmoid layer --> resulting in 10x1 vector\r\n    dC::map_by_element(firstLayerOutput,&dC::sigmoid);\r\n    //    apply sofmax layer --> resulting in 10x1 vector\r\n    softmax(firstLayerOutput);\r\n    //    display the first output layer and its Jaccobian\r\n    //    Jacobian is a 10x7840 matrix of derivatives\r\n    for (size_t i=0, nRows=firstLayerOutput.rows(), nCols=firstLayerOutput.cols(); i<nCols; ++i){\r\n                for (size_t j=0; j<nRows; ++j) {\r\n                    dC::print(firstLayerOutput(j,i));\r\n                }\r\n                cout<<endl;\r\n    }\r\n}\r\n\r\n```\r\n\r\n<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">dC++</span> by <a xmlns:cc=\"http://creativecommons.org/ns#\" href=\"https://si.linkedin.com/in/zigasajovic\" property=\"cc:attributionName\" rel=\"cc:attributionURL\">Å½iga Sajovic</a> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>.\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}