\documentclass{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{bbm}

\usepackage[T1]{fontenc}
\usepackage{inconsolata}

\usepackage{tikz}
\usepackage{tikz-cd}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{positioning}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{pifont}

\usepackage{color}
\definecolor{bluekeywords}{rgb}{0.13,0.13,1}
\definecolor{greencomments}{rgb}{0,0.5,0}
\definecolor{redstrings}{rgb}{0.9,0,0}

\usepackage{listings}

\lstset{language=[GNU]C++,
showspaces=false,
showtabs=false,
breaklines=true,
showstringspaces=false,
breakatwhitespace=true,
escapeinside={(*@}{@*)},
commentstyle=\color{greencomments},
keywordstyle=\color{bluekeywords}\bfseries,
stringstyle=\color{redstrings},
basicstyle=\ttfamily
}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\Shift}{\mathcal{S}}
\newcommand{\II}{\mathbb{I}}
\newcommand{\JJ}{\mathbb{J}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\VV}{\mathcal{V}}
\newcommand{\MM}{\mathcal{M}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\uu}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\CC}{C\nolinebreak\hspace{-.05em}\raisebox{.4ex}{\tiny\bf +}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\tiny\bf +}}
\def\CC{{C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\tiny\bf ++}}}
\newcommand{\dP}{\mathcal{P}}
% operator odvoda
\newcommand{\D}{\partial}
%operator 1 + \D
\newcommand{\Dplus}{\mathcal{D}}
% operator 1+ \D + \D^2 + ...
\newcommand{\sumd}{\tau}
\newcommand{\Op}{\partial^{\bigoplus}}
\newcommand{\op}[1]{\partial^{#1\bigoplus}}
\DeclareMathOperator{\interior}{int}


\newtheorem{definicija}{Definition}[section]
\newtheorem{trditev}{Claim}[section]
\newtheorem{izrek}{Theorem}[section]
\newtheorem{opomba}{Remark}[section]
\newtheorem{corollary}{Corollary}[section]

\usepackage[sorting=none,backend=bibtex]{biblatex}
\bibliography{biblio}


\title{Operational calculus and algebra of Algorithms}
\author{Å½iga Sajovic, Martin Vuk}
\begin{document}
\maketitle
\begin{abstract}
In this paper, we develop the theory of analytic programming languages, that
implements spaces of infinitely-differentiable programs and operators acting upon them.

A programming space is a subalgeba of the function algebra of maps on the virtual
memory. We can construct a differential operator on the programming spaces as we 
model virtual memory as a tensor product of a vector space with tensor algebra
of its dual. Virtual memory serves by its own as an algebra of polynomial
programs, giving the approximation of the original program as a Taylor series at
program's input values. 

We present a theory of operators on programming spaces, that enables analysis of programs
and computations on the operator level, which favors general implementation. We
also present several examples of how the theory can be used in computer science.

The theory opens new doors in program analysis. We develop a general
procedure which takes a program that tests an object for a property and
constructs a procedure that enforces that property upon any object.
We use the theory to analyze iterators and other control structures in programming
languages, and their dependencies on boundary conditions. The theory enables
approximation of programs and enables us to choose programs' complexity, 
while knowing the order of the error of the approximation.

\end{abstract}

\section{Introduction}
Programming holds the power of algorithmic control flow and freedom of expression, whose abstinence severely limits descriptiveness of closed form methods of \textit{pen and paper} mathematics, thus firmly cementing programming as the language of modern problem solving. Yet, a vibrant tradition of mathematics has existed since the dawn of mind, that remains, with the exception of combinatorics, largely untapped by computer science. 

Just as the discrete nature of physical reality is studied through analytic means, so can the nature of digital phenomena. Studying these procedures as objects undergoing change in some virtual space, has partially been attempted in some fields, such as Hamiltonian Monte Carlo methods of Bayesian predictors, that Girolami and Calderhead \cite{StatMC} studied as manifolds to great success, using unfortunately impractical methods of hard-coding derivatives of distributions. This of course stifles the freedom of algorithmic expression programming is embraced for.

The way evaluation of algorithmic expressions differs from evaluation of symbolic expressions of standard analysis, lies at the very core of this dissonance. The disconnect was partially mitigated by methods of automatic differentiation, utilized today in machine learning, engineering, simulations, etc. Yet under the lack of a proper formalism the model collapses when one tries to generalize to such notions as a differentiable program $p_1$ operating on (differentiable) derivatives of another program $p_2$ (where only coding of $p_2$ is required within $p_1$), while retaining the famed freedom of expression. 
Models allowing for nested differentiation \cite{AD1}, still fail in providing algorithms with an algebra enabling study and formulation of programs through equations. Thus, they remain nothing more than means to calculating derivatives, void of any meaningful algebraic insight, lacking the true power the vast field of analysis is endowed with.

\section{Computer programs as maps}
We will model computer programs as maps on a vector space. If
we only focus on the real valued variables (of type \texttt{float} or
\texttt{double}),  the state of the virtual memory can be seen as a high
dimensional vector\footnote{we assume the variables of interest to be of type \texttt{float} for
  simplicity. Theoretically any field can be used instead of $\RR$.}. 
A set of all the possible states of the program's memory,
can be modeled by a finite dimensional real vector space $\VV\equiv \RR^n$. We
will call $\VV$ \emph{memory space of the program}. The effect of a computer
program on its memory space $\VV$, can be described by a map
\begin{equation}
  \label{eq:map}
  P:\VV\to \VV.
\end{equation}
We can compose two programs sequentially, to obtain another program, which
results in map composition. The set of all such maps $\F=\VV^\VV$ is a
\emph{monoid} for the operation of composition $\circ$. A programing language is
a set of elementary operations that are sequentially applied. 
\begin{definicija} A \emph{multivariate/Euclidean/Hilbert/vector?? computing machine} is a pair $(\VV,\E)$, such
  that $\VV$ is a finite dimensional vector space over a filed $F$ and
  $\E\subset \VV^\VV$ is a set of maps, called \emph{elementary operations}. A
  \emph{program} is any map $P:\VV\to \VV$, that can be written as a finite  composition
  of elementary operations
  \begin{equation}
    \label{eq:P_composition}
    P=\varepsilon_k\circ\ldots \varepsilon_1;\quad \varepsilon_i\in \E
  \end{equation}
  
\end{definicija}

Denote by $e_1,\ldots e_n$ a standard basis of the memory space $\VV$ and by
$x_1,\ldots x_n$ the dual basis of $\VV^*$. The functions $x_i$ are coordinate
functions on $\VV$ and correspond to individual variables in the memory. For
each program $P:\VV\to \VV$ we divide variables $x_i$ into three groups. Let us
denote with $P_i$ the $i-th$ component of the map $P$ according to the standard
basis. Function $P_i:\VV\to F$ is a scalar function and is equal to $x_i\circ P$.
\begin{definicija}
  Let $x_i$ be the variable in memory and $P:\VV\to\VV$ a program.  
  \begin{enumerate}
  \item 
    The variable $x_i$ is called \emph{free}, if
    \begin{equation}
      \label{eq:free}
      \frac{\partial P_j}{\partial x_i}\equiv \delta_{ij}\wedge \frac{\partial P_i}{\partial x_j}\equiv \delta_{ij}.
    \end{equation}
\item 
    The variable $x_i$ is called \emph{input parameter} if
    \begin{equation}
      \label{eq:input}
      \frac{\partial P_j}{\partial x_i}\not\equiv \delta_{ij}.\footnote{this statement also includes the case that the partial derivative do not exist at some point}
    \end{equation}
\item  The variable $x_i$ is called \emph{output value} if
    \begin{equation}
      \label{eq:output}
      \frac{\partial P_i}{\partial x_j}\not\equiv \delta_{ij} 
    \end{equation}
  \end{enumerate}

\end{definicija}
Free variables are those that are left intact by the program and have no
influence on the final result. The output of the program depends only on the
values of the input variables and consists of variables that are changed during
the program. Input parameters and output values might overlap. 
We also define corresponding vector subspaces of the program $P$.
\begin{definicija}
  The \emph{input} or \emph{parameter space} is a vector subspace $I_P<\VV$,
  spanned by the standard basis vectors, that correspond to \emph{input}
  variables. Similarly  \emph{output space} is a subspace
  $O_P<\VV$ is spanned by basis vectors corresponding to output variables and
  \emph{free space} is a subspace $F_P<\VV$ spanned by the basis vectors
  corresponding to free variables. 
\end{definicija}
The input and the output space of a program $P$ are the minimal
vector subspaces spanned by the standard basis vectors, such that the map $P_e$, 
defined by the following commutative diagram 
\begin{equation} 
    \label{eq:induced_map}
\begin{tikzcd}
  \VV \arrow{r}{P} & 
  \VV \arrow{d}{\mathrm{pr}_{O_P}}\\
  I_P \arrow[hook]{u}{\vec{i}\mapsto \vec{i}+\vec{f}} 
  \arrow{r}{P_e}& O_P
\end{tikzcd}
  \end{equation}
does not depend of the choice of the element 
$\vec{f}\in F_P=(I_P+O_P)^\perp$. The map $P_e$ describes the actual effect of the
program $P$, since it ignores the free memory. 
\section{Multivariate derivative}
\begin{definicija}[Derivative]
  Let $V,U$ be Banach spaces. The map $P:V\to U$ is differentiable at the point
  $x\in V$, if there exists a linear bounded operator $TP_x:V\to U$ such that
  \begin{equation}
    \label{eq:frechet}
    \lim_{h\to 0}\frac{\|P(x+h)-P(x)-TP_x(h)\|}{\|h\|} = 0.
  \end{equation}
  The map $TP_x$ is called the \emph{FrÃ©chet derivative} of the map $P$ at the
  point $\x$.
\end{definicija}
For maps $\RR^n\to \RR^m$ FrÃ©chet derivative can be expressed by multiplication
of vector $h$ by the Jacobi matrix $\D P$ of the  partial derivatives of the 
components of the map $P$
\begin{equation*}
  TP_x(h) = \D P(x)\cdot h.
\end{equation*}
We assume for the remainder of this section that the map $P:V\to U$ is
differentiable for all $\x\in V$. The derivative defines a map from $V$ to
linear bounded maps from $V$ to $U$. We further assume $U$ and $V$ are finite
dimensional. Then the space of linear maps from $V$ to $U$ is isomorphic to
tensor product $U\otimes V^*$. For a simple tensor $\uu\otimes f\in U\otimes
V^*$ one can define a linear map by the following
 \begin{equation}
   \label{eq:lin_tenzor}
   \x \mapsto f(\x)\cdot \uu.
 \end{equation}
This map, extended by linearity to the whole tensor product $U\otimes V^*$ is an
isomorphism. The derivative defines a map
\begin{eqnarray}
  \label{eq:odvod_preslikava}
  \D P&:& V\to U\otimes V^*\\
  \D P&:& \x \mapsto T_\x P.
\end{eqnarray}
As the derivative defines a map $\D P:V\to U\otimes V^*$, one can consider its
differentiability. The derivative of the map $\D P$ is the \emph{second derivative}
of the map $P$ and is denoted by $\D^2P$. Second derivative is at each point a
linear map  $V\to U\otimes V^*$, and can be therefore 
represented by a tensor from $(U\otimes V^*)\otimes V^*$. Second derivative
defines a map 
$$\D^2 P:V\to U\otimes V^*\otimes V^*.$$ 

Similarly higher derivatives can be defined as maps.
\begin{definicija}[higher derivatives]
  Let $P:V\to U$ be a map from vector space $V$ to vector space $U$. 
The derivative of order $k$ is a map $\D^kP:V\to U\otimes(V^*)^{\otimes k}$,
that maps $\x\in V$ to the derivative  $T_\x(\D^{k-1}P)$ of the $k-1$-th order
derivative as a map $\D^{k-1}P: V\to U\otimes (V^*)^{\otimes k-1}$:
  \begin{eqnarray}\label{eq:partial}
    \label{eq:visji_odvod}
    \D^kP&:&V\to U\otimes (V^*)^{\otimes k}\\
    \D^kP&:&\x\mapsto T_\x\left( \D^{k-1}P \right)
  \end{eqnarray}
\end{definicija} 
\begin{opomba}
  For the sake of clarity, we assumed in the definition above, that the map $P$ as well as all its
  derivatives are differentiable at all points $\x$. If this is not the case
  definitions above can be done locally, which would introduce mostly technical difficulties.
\end{opomba}
Let $\e_1,\ldots,\e_n$ be a basis of $U$ and $x_1,\ldots x_m$ the basis of
$V^*$. Then $\D^k$ $\eqref{eq:partial}$ can be defined in terms of
directional(partial) derivatives by the formula
\begin{equation}\label{eq:dd}
	\partial^k=\sum_{\forall_{i,\alpha}}\frac{\partial^k}{\partial
	    x_{\alpha_1}\ldots \partial x_{\alpha_k}}\e_i\otimes
	  dx_{\alpha_1}\otimes\ldots \otimes dx_{\alpha_k}.
\end{equation}

\section{Tensor algebra representation}
In order to apply the definition of derivative of a program to higher
derivatives, we use the tensor algebra.
Let $P:V\to U$ be a map on vector spaces. Motivated by definition of higher
derivatives, we define a sequence of spaces with
recursive formula
\begin{eqnarray}
  \label{eq:universal_space}
  U_0 &=& U\\
  U_k &=& U_{k-1}+\left(U_{k-1}\otimes V^*\right).
\end{eqnarray}
Note that the sum is not direct, since some of the subspaces of $U_{k-1}$ and
$U_{k-1}\otimes V^*$ are naturally isomorphic and will be identified.
The formula can be illustrated as follows. 
To implement the derivative of a map $P:V\to U$, the space for $U\oplus
(U\otimes V^*)$ has to be allocated.
Denote $U_k$ the space needed for $k$-th order derivative. The program for
$k-th$ derivative would implement the map $V\to U_k$. For the derivative of this map
the space $U_k+ (U_k\otimes V^*)$ would have to be allocated. This sum
however would not be direct, since some of the subspaces of $U_k$ coincide with
subspaces of $U_k\otimes V^*$.

The space 
\begin{equation}
\label{eq:k-th-virtual-space}
U_k = U\otimes \left(K\oplus V^* \oplus (V^*\otimes V^*)\oplus\ldots
(V^*)^{\otimes k}\right) = U\otimes T_k(V^*)
\end{equation}
satisfies the recursive formula (\ref{eq:universal_space}). The space $T_k(V^*)$
is a subspace of \emph{tensor algebra}, consisting of linear combinations of tensors of rank less
or equal $k$. To show, that $U_k$ is indeed $U\otimes T_k(V^*)$, we use
recursive formula
\begin{multline*}
  U_{k+1}=U\otimes(K\oplus V^*\oplus\ldots (V^*)^{\otimes (k+1)})=\\
 U\oplus (U\otimes V^*)\oplus\ldots (U\oplus U\otimes(V^*)^{\otimes (k+1)})
\end{multline*}
on the other hand, using recursive formula
\begin{multline}
  U_k+(U_k\otimes V^*) = \left( U\oplus (U\otimes V^*)\oplus\ldots (U\otimes(V^*)^{\otimes k})\right) + \\
  \left((U\otimes V^*) \oplus (U\otimes V^*\otimes V^*) \oplus\ldots (U\otimes(V^*)^{\otimes k}\otimes V^*)\right)
\end{multline}
However spaces $U\otimes(V^*)^{\otimes (j+1)}$ and $U\otimes (V^*)^{\otimes
  j}\otimes V^*$ are naturally isomorphic and will be identified, leaving 
\begin{equation}
  U_k+(U_k\otimes V^*) = U\oplus (U\otimes V^*)\oplus\ldots (U\otimes(V^*)^{\otimes (k+1)})
\end{equation}
The spaces $U_k$ form an increasing sequence of subspaces (a flag) of the space $U\otimes
T(V^*)$
\begin{equation}
  \label{eq:flag}
  U_0<U_1<U_2<\ldots < U\otimes T(V^*).
\end{equation}
This enables us to define all the derivatives as maps with
the same domain and codomain $V\to U\otimes T(V^*)$. Indeed for any map $P:V\to
U$, one can define a map to $U\otimes T(V^*)$ as
$$V\xrightarrow{P}U\hookrightarrow U\otimes T(V^*)$$ and similarly  for all of its
derivatives
$$V\xrightarrow{\D^k P} U\otimes (V^*)^{\otimes k}\hookrightarrow U\otimes
T(V^*).$$
\subsection{Differentiable programs}
To each program $P:\VV\to \VV$ one can attach the effective map $P_e:I_P\to
O_P$. The derivative of this map is of interest, when we speak about
differentiability of computer programs. 
\begin{definicija}[Automatically differentiable programs]
  A program $P:\VV\to \VV$ is \emph{automatically differentiable} if there exist an embedding
  $O_P\otimes I_P^*\hookrightarrow F_P$ of the space $O_P\otimes I_P^*$ into
  free space $F_P$, and a program $\D P:\VV\to \VV$, such that its effective
  map is the map
  \begin{equation}
    \label{eq:program_derivative}
    P_e\oplus \D P_e:I_P\rightarrow O_P\oplus (O_P\otimes I^*).
  \end{equation}
The vector computing machine $(\VV, \E)$ is \emph{automatically differentiable}
if all of the elementary operations $\varepsilon\in\E$ are \emph{automatically
  differentiable}.
\end{definicija}

If a program $P:\VV\to \VV$ is automatically differentiable then it is also
differentiable as a map $\VV\to\VV$. However only the derivative of program's
effective map can be implemented as a program, since the memory space is limited to $\VV$. 

Putting memory considerations aside, we propose an universal model of the memory
space to be 
$$\VV_\infty = \VV\otimes T(\VV^*) = \VV\oplus
(\VV\otimes\VV^*)+\ldots.$$
We extend each program $P:\VV\to
\VV$ to the map on universal memory space by setting the first component to $P$,
and all other components to zero. Each element of $\VV_\infty$ can be written
uniquely as a sum $\vv_0+T_1+T_2+\ldots$, where $\vv_0\in\VV$ and 
$T_i\in\VV\otimes (\VV^*)^{\otimes i}$ and the extended map acts on it as follows   
\begin{equation}
  \label{eq:extension}
  P(\vv_0+T_1+T_2+\ldots)=P(\vv_0)+0+0+\ldots
\end{equation}
\section{Operational calculus and algebra of Algorithms}

The derivative defines an operator on the space of smooth maps $V\to U\otimes
 T(V^*)$\footnote{the operator $\D$ may be defined partially for other maps as
   well.}. We denote this operator $\D$. If we look at a map $P:V\to U$ as a map
 to $U\otimes T(V^*)$, then the image of $P$ by operator $\D$ is the first
 derivative, while the higher order derivatives are just powers of operator $\D$
 applied to $P$. 

Let us define the following function spaces.
 \begin{equation}\label{eq:F^n}
 	\F^n=\{f:V\to U\otimes(V^*)^{n\otimes}\}
 \end{equation}
 and
 \begin{equation}\label{eq:F_n}
 	\F_n=\{f:V\to U\otimes T_n(V^*)\}
 \end{equation}
All of these function spaces are subspaces of $\F_\infty=\{f:V\to U\otimes
T(V^*)\}$. The space $\F_n$ is direct a sum of $\F^k$ for $k\le n$ and $\F_n<\F_m$ if $n<m$. We
have an increasing sequence of function spaces
\begin{equation}
  \label{eq:flag_function_space}
  \F_0<\F_1<\ldots<\F_n<\ldots
\end{equation}

Thus $\D^k$ is a mapping between function spaces $\eqref{eq:F^n}$
 
 \begin{equation}\label{eq:toFn+k}
 \D^k:\F^n\to\F^{n+k}
 \end{equation}
 
 \begin{izrek}\label{izr:linearnaNeodvisnost}
  The set $\{\partial^k\}$ represents linearly independent vectors over a field $K$.
 \end{izrek}
 
 \begin{definicija}\label{def:dP}
 	A differentiable programming space $\dP_0$ is any subspace of $\F_0$ satisfying the relation
 	\begin{equation}\label{eq:P}
 	\dP_0<\F_0\iff\D\dP_0\subset\dP_0\otimes T(V^*)
 	\end{equation}
 \end{definicija}

\begin{izrek}\label{izr:P}
	Any programming language $\dP_0$ satisfying Definition $\ref{def:dP}$ is an infinitely differentiable programming space, satisfying the relation
	\begin{equation}\label{eq:P_n}
	 		\dP_0<\F_0\iff\D^k\dP_0\subset\dP_0\otimes T(V^*)
	 	\end{equation}
\end{izrek}
\begin{proof} Induction hypothesis follows from Definition $\ref{def:dP}$
	$$\dP_0<\F_0\iff\D\dP_0\subset\dP_0\otimes T(V^*)$$
	Induction step is trivial
	\begin{equation}\label{eq:inductionStep}
	\forall_{p\in\dP_0}\Big(\D^{n+1}p^i_{j,k}=\D(\D^n p^i_j)_k\land(\D p)^i_j\in\dP_0\Big)
	\end{equation}
	$$\implies$$
	$$\dP_0<\F_0\iff\D^k\dP_0\subset\dP_0\otimes T(V^*)$$
\end{proof}

 \begin{izrek}\label{izr:dP}
	A programming space $\dP=\{\dP:V\to U\otimes T(V^*)\}$, is closed under the differential operator $\D^k$.
 \end{izrek}
 \begin{proof}
 	 By $\eqref{eq:P_n}$ and the symmetric nature of the operator $\D$, for each $k$ there exists an isomorphism
 	 	\begin{equation}
 	 		\dP_0\to\D^k\dP_0
 	 	\end{equation}
 \end{proof}

\begin{corollary}\label{cor:dpMonoid}
The space $\dP$ is closed under the powers of $\D$.
\end{corollary}

By Theorem $\ref{izr:dP}$ we may represent calculation of derivatives of the map $P:V\to U$, with only one mapping. We define the geometric series
 
 \begin{equation}\label{eq:DD}
  	\sumd_n = 1+\D +\D^2 +\ldots + \D^n 
  \end{equation}
  
  
  \begin{equation}
  	\sumd_n=\sum\limits_{n=0}^{n}\sum_{\forall_{i,\alpha}}\frac{\partial^n}{\partial
  		    x_{\alpha_1}\ldots \partial x_{\alpha_k}}\e_i\otimes
  		  dx_{\alpha_1}\otimes\ldots \otimes dx_{\alpha_k} , x_i\in V_{j\in\JJ}
  \end{equation}
  
  By Theorem $\ref{izr:linearnaNeodvisnost}$ the operator $\sumd_n$ is unique and prescribes the map $P: V\to U$ with a map $\sumd_k:V\to
U\otimes T(V^*)$, composed of the map itself and all derivatives of order $n\le k$. The image $\sumd_kP(\x)$ is a multitensor of order $k$, which is a direct sum of the maps value and all derivatives of order $n\le k$, all evaluated at the point $\x$:
\begin{equation}
  \label{eq:multi_odvod}
  \sumd_kP(\x) = P(\x)+\D_\x P + \D^2_\x P + \ldots + \D^k_\x P.
\end{equation}
\begin{izrek}\label{izr:tauRek}
  The operator $\sumd_k$ is recursively defined with the expression
  \begin{equation}
    \label{eq:potenca(1+d)}
    \sumd_{k+1}=1+\D\sumd_{k}
  \end{equation}
and starting condition $\tau_0=1$.
\end{izrek}
There exists a simple recursive relation between $\sumd_k$ and $\sumd_{k+1}$
\begin{equation}
   \label{eq:rekurzija}
   \sumd_{k+1} = 1 + \D +\D^2+\ldots \D^{k+1} = 1+\D(1+\D+\ldots +\D^{k}) = 1+\D\sumd_k.
\end{equation} 

We seek to generalize such notions.

\begin{definicija}\label{def:P_n}
Let $\dP_n$ be the subspace, spanned by $\{\partial^n\}$ over $K$.
 \end{definicija}
 
 \begin{izrek}\label{izr:P_n}
 	Function space $\dP_n:V\to U\otimes T(V^*)$ is isomorphic to the tensor product of the function space $\dP_0:V\to U$ and tensor algebra $T(V^*)$ of the dual od the virtual space $V$.
 	
 	\begin{equation}
 	\label{eq:P_algebra}
 	 	    \dP\infty\simeq \dP_0\otimes T(V^*) = \dP_0 \otimes\left(\bigoplus_{k=0}^\infty (V^*)^{\otimes k} \right)
 	\end{equation}
 \end{izrek}
 
 \begin{proof} Follows directly from the proof of Theorems $\ref{izr:P}$ and $\ref{izr:dP}$ through argument $\eqref{eq:inductionStep}$.
  \end{proof}
  \begin{corollary}
  If there exists a construction of a first order programming space $\dP_1$, there exists a construction of a programming space of an arbitrary order $\dP_n$.
  \begin{equation}
  \exists_{\sumd}(\sumd:\dP_0\to\dP_1)\iff\forall_n\exists_{\sumd_n}(\sumd_n:\dP_0\to\dP_n)
  \end{equation}
  \end{corollary}
      \begin{corollary}\label{rem:vTen}
       Maps $V\otimes T(V^*)\to V\otimes T(V^*)$ are constructible through tensor algebra on $\dP_n$ and through it, compositions of programs in $\dP_n$. As such, it is heron assumed to be a monoid.
       \end{corollary}
       
    The following
     \begin{equation}
     	\sumd_n\in\dP_0\otimes T_n(V^*)
     \end{equation}
   	holds by Theorem $\ref{izr:P_n}$, allowing simple implementation as dictated by expression $\eqref{eq:potenca(1+d)}$.
        \begin{opomba}
        Of course the same holds true for all generating maps of the type
        $$\dP_0\to\dP_n$$
        \end{opomba}
        
\begin{izrek}\label{izr:reductionMapSingle}
There exists a reduction of order map $\phi:\dP^n\to\dP^{n-1}$, satisfying the relation
\begin{equation}\label{eq:reductionMapSingle}
\forall_{p\in\dP}\Big(\phi(\D^kp)=\D\phi(\D^{k-1}p)\Big)
\end{equation}
for each $k\ge 1$.
\end{izrek}        

Now we define a product on $\dP_n$. By Theorem $\ref{izr:P_n}$ it holds that

\begin{equation}
	f,g\in\dP\implies\exists_{p\in\dP}\Big(\D^nf\D^mg=\D^{n+m}p\Big)
\end{equation}
The set $\{\D^n\}$ implies nilpotency $k>n\implies\D^k=0$. The product between two elements of $\dP_n$ is implicitly defined with its' tensor algebra. 
\begin{equation}\label{eq:P_prod}
	\sum\limits_{i=0}^{n}\D^if\sum\limits_{j=0}^{n}\D^jg=\sum\limits_{k=0}^{n}\sum\limits_{i+j=k}\D^if\D^jg
\end{equation}

\begin{izrek}[Algebra of Algorithms]\label{izr:alg}
An infinitely-differentiable programming space $\dP_n$ is a function algebra, with the product defined by $\eqref{eq:P_prod}$.
\end{izrek}

\subsection{Analytic programming languages}
Algorithms may be interpreted as a functional algebra over $\dP_0:V\to V$.
By Theorem $\ref{izr:P_n}$, we can express $\dP_n$, that is spanned by $\{\D^n\}$ over $V$, through linear combinations of elements of $\dP_0\otimes T(V^*)$; isomorphism $\eqref{eq:P_algebra}$, generated by the tuple $(\dP_0,V)$, equipped with tensor algebra. Thus the tuple  $(\dP_0,V)$ and the belonging algebra are sufficient conditions for the existence and construction of infinitely differentiable programing spaces $\dP_n$ $\eqref{def:P_n}$, which are function algebras by Theorem $\ref{izr:alg}$.

\begin{definicija}[Analytic programming language]
The tuple $M=\langle\dP, V\rangle$ equipped with tensor algebra, is an analytic, infinitely  differentiable programming language.
   
    \begin{itemize}
    \item
    $V$ is a virtual space, "infinite tape"
    \item
    $\dP$ is an algebra monoid satisfying Definition $\ref{def:dP}$
    \end{itemize}
  \end{definicija}
Anything that operates under these specifications is an analytic programming language, with the ability to implement infinitely differentiable programming spaces $\dP_\infty$.

 \subsection{Power series expansion}\label{sec:Vrsta}
 
We motivate the following with an example. Suppose an algorithm of high time complexity. As speed is of importance, an approximation would suffice. Using the described mechanics, we may linearize the program. We choose a $v_0\in\dP$ and linearize $P'=P(v_0)+\D P(v_0)\cdot(v-v_0)$. The latter has to be done only once.
 
 Locally, the image of the linearization is a good approximation of the original program. To extend this radius, we would like to expand said program into a series. By Theorem $\ref{izr:linearnaNeodvisnost}$  there exists $\dP_n$, that is spanned by the set $\{\D^k\}$ over $\dP$ (which is itself a function space over $V$). Thus, the expression
 \begin{equation}
 	e^{h\D}=\sum\limits_{n=0}^{\infty}\frac{(h\D)^n}{n!}
 \end{equation}
 is well defined. In coordinates, the operator $e^{h\D}$ can be written as a
 series over all multi-indices $\alpha$
 \begin{equation}\label{eq:e^d}
 	e^{h\D}=\sum\limits_{n=0}^{\infty}\frac{h^n}{n!}\sum_{\forall_{i,\alpha}}\frac{\partial^n}{\partial
 		    x_{\alpha_1}\ldots \partial x_{\alpha_n}}\e_i\otimes
 		  dx_{\alpha_1}\otimes\ldots \otimes dx_{\alpha_n}.
 \end{equation}
The operator $e^{h\D}$ is a mapping between function spaces $\eqref{eq:F_n}$
 \begin{equation}
 	e^{h\D}:\dP\to\dP_\infty.
 \end{equation}
 It also defines a map
  \begin{equation}\label{eq:specProg}
  	e^{h\D}:\dP\times V\to U\otimes T(V^*),
  \end{equation}
by taking the image of the map $e^{h\D}(P)$ at a certain point $\vv\in V$.  
Through $\eqref{eq:specProg}$ we may construct a map from the space of programs,
to the space of polynomials. Note that the space of multivariate polynomials
$V\to K$ is isomorphic to symmetric algebra $S(V^*)$, which is in turn a
quotient of tensor algebra $T(V^*)$. To any element of
 $U\otimes T(V^*)$ one can attach corresponding element of $U\otimes S(V^{i*})$
 namely a polynomial map  $V\to U$.
 \begin{equation}\label{eq:pToPol}
 	e^{h\D}: \dP\times V\to U\otimes S(V^*)
 \end{equation}
 For any element $\vv_0\in V$, the expression $e^{h\D}(\cdot,\vv_0)$ is a map $\dP\to
 U\otimes S(V^*)$, mapping a program to a polynomial. We can express the
 correspondence between multi-tensors in $U\otimes T(V^*)$ and polynomial maps
 $V\to U$ given by multiple contractions for all possible indices. For a simple tensor $\uu\otimes
 f_1\otimes\ldots\otimes f_n\in U\otimes(V^*)^{\otimes n}$ the contraction by
 $\vv\in V$ is given by applying co-vector $f_n$ to $\vv$ 
 \begin{equation}
   \label{eq:contraction}
 \uu\otimes f_1\otimes\ldots\otimes f_n\cdot \vv = f_n(\vv) \uu\otimes f_1\otimes\ldots f_{n-1}.
 \end{equation}
Applying contraction multiple times with the same vector $\vv$ one can assign to
any simple tensor a monomial map 
 \begin{equation}
   \label{eq:tensor->poly}
 \uu\otimes f_1\otimes\ldots\otimes f_n: \vv \mapsto f_n(\vv)\cdots f_1(\vv) \uu
 \end{equation}

and by linearity to any finite rank multi-tensor in $U\otimes T(V^*)$ a
polynomial map.
 
\begin{izrek}\label{izr:e^d}
	For a program $P\in\dP$  the expansion into a Taylor series
  at the point $\vv_0\in V$ is expressed by multiple contractions 
	\begin{multline}\label{eq:tenzorVrsta}
	P(\vv_0+h\vv) = \Big((e^{h\D}P)(\vv_0)\Big)\cdot\vv
  = \sum_{n=0}^\infty\frac{h^n}{n!}\D^nP(\vv_0)\cdot (\vv^{\otimes n})\\
  = \sum_{n=0}^\infty \frac{h^n}{n!}\sum_{\forall_{i,\alpha}}\frac{\partial^nP_i}{\partial
 		    x_{\alpha_1}\ldots \partial x_{\alpha_n}}\e_i\cdot
 		  dx_{\alpha_1}(\vv)\cdot\ldots \cdot dx_{\alpha_n}(\vv).
	\end{multline}
\end{izrek}
 
 \begin{proof}
We will show that $\frac{d^n}{dh^n}\text{(LHS)}|_{h=0}=\frac{d^n}{dh^n}\text{(RHS)}|_{h=0}$. Then $\text{LHS}$ and $\text{RHS}$ as functions
of $h$ have coinciding Taylor series and are therefore equal.\\
 $\implies$
 
 $$\frac{d^n}{dh^n}P(\vv_0+h\vv)|_{h=0}=\D^n P(\vv_0)(\vv)$$
 $\impliedby$
 $$\lim\limits_{\lVert h\rVert\to 0}(\frac{d}{dh})^n\Big((e^{h\D})(P)(V)\Big)(v^{\JJ_k}_{k})=\lim\limits_{\lVert h\rVert\to 0}\Big((\D^n e^{h\D})(P)(V)\Big)(v^{\JJ_k}_{k})$$
 $$\land$$
 $$\lim\limits_{\lVert h\rVert\to 0}\D^ne^{h\D}=\lim\limits_{\lVert h\rVert\to 0}\sum\limits_{i=0}^{\infty}\frac{h^i\D^{i+n}}{i!}=\D^n$$
 $$\implies$$
 $$\Big(\D^n(P)(V)\Big)(v^{\otimes n})$$
 \end{proof}
 
 The image of the contraction is an element of the original virtual space $V^i$. Independence of the operator $(\ref{eq:specProg})$ from a coordinate system, translates to independence in execution. Thus the expression $(\ref{eq:tenzorVrsta})$ is invariant to the point in execution of a program, a fact we explore later on.  
 
 Theorem $\ref{izr:P_n}$ implies
     \begin{equation}
     	e^{h\D}\in\dP_0\otimes T(V^*)
     \end{equation}      
which enables efficient implementation.
 
 \begin{izrek}\label{izr:prod}
 The operator $e^{h\D}$ is an automorphism of the algebra over $\dP_\infty$
 \begin{equation}
 	e^{h\D}(p_1\cdot p_2)=e^{h\D}(p_1)\cdot e^{h\D}(p_2)
 \end{equation}
 where $\cdot$ stands for a bilinear map.
 \end{izrek}
 
 \begin{proof}
 It is true for all $n$ that $\lim\limits_{\lVert h\rVert\to 0}(\frac{d}{dh})^n\text{(LHS)}=\lim\limits_{\lVert h\rVert\to 0}(\frac{d}{dh})^n\text{(RHS)}$\\
  $\implies$
  $$\lim\limits_{\lVert h\rVert\to 0}(\frac{d}{dh})^ne^{h\D}(p_1\cdot p_2)=\lim\limits_{\lVert h\rVert\to 0}\D^ne^{h\D}(p_1\cdot p_2)$$
  $$\implies$$
  $$\D^n(p_1\cdot p_2)$$
  $\impliedby$
  $$\lim\limits_{\lVert h\rVert\to 0}(\frac{d}{dh})^n\Big(e^{h\D}(p_1)\cdot e^{h\D}(p_2)\Big)=\lim\limits_{\lVert h\rVert\to 0}\sum\limits_{k=0}^{n}{n\choose k}\D^{n-k}e^{h\D}(p_1)\cdot \D^ke^{h\D}(p_2)$$
  $$\implies$$
  $$\sum\limits_{k=0}^{n}{n\choose k}\D^{n-k}p_1\cdot \D^kp_2$$
 \end{proof}
 
 \begin{izrek}\label{izr:kompo}
 Composition of maps $\dP$ is expressed as
 \begin{equation}\label{eq:kompo}
 e^{h\D}(f\circ g)=exp(\D_fe^{h\D_g})(g)(f)
 \end{equation}
 \end{izrek}
 
\begin{proof}
 It is true for all $n$ that $\lim\limits_{\lVert h\rVert\to 0}(\frac{d}{dh})^n\text{(LHS)}=\lim\limits_{\lVert h\rVert\to 0}(\frac{d}{dh})^n\text{(RHS)}$\\
 $\implies$
 $$\lim\limits_{\lVert h\rVert\to 0}(\frac{d}{dh})^ne^\D(f\circ g)=\lim\limits_{\lVert h\rVert\to 0}\D^ne^{h\D}(f\circ g)$$
 $$\implies$$
 $$\D^n(f\circ g)$$
 $\impliedby$
 $$exp(\D_fe^{h\D_g})=\prod_{i=1}^{\infty}e^{\D_f\frac{(h\D_g)^i}{i!}}\Big(e^{\D_f}\Big)$$
 $$\implies$$
 $$exp(\D_fe^{h\D_g})(g)(f)=\sum\limits_{\forall_n}h^n\sum\limits_{\lambda(n)}\prod\limits_{k\cdot l\in\lambda}\Big(\frac{\D_f\D_g^l(g)}{l!}\Big)^k\frac{1}{k!}\Big(\Big(e^{\D_f}\Big)f\Big)$$
 where $\lambda(n)$ stands for the partitions of $n$.
 $$\lim\limits_{\lVert h\rVert\to 0}(\frac{d}{dh})^nexp(\D_fe^{h\D_g})=\lim\limits_{\lVert h\rVert\to 0}\sum\limits_{\forall_m}n!h^{n-m}\sum\limits_{\lambda(m)}\prod\limits_{k\cdot l\in\lambda}\Big(\frac{\D_f\D_g^l(g)}{l!}\Big)^k\frac{1}{k!}\Big(\Big(e^{\D_f}\Big)f\Big)$$
 $$\implies$$
 \begin{equation}\label{eq:dComposite}
 \sum\limits_{\lambda(n)}\prod\limits_{k\cdot l\in\lambda}\Big(\frac{\D_f\D_g^l(g)}{l!}\Big)^k\frac{n!}{k!}\Big(\Big(e^{\D_f}\Big)f\Big)
 \end{equation}
 where $e^{\D_f}$ maps $\dP_0\to\dP_\infty$ enabling evaluation at a point. 
 \end{proof}       
 \begin{opomba}
 As such it may be convenient to apply $g(v)\in V$ to $f$ before hand, as $e^\D$ applied to a constant represents the identity map and grants us the derivative $\D^n$ at a point $v\in V$ instead of an expansion into a multitensor (onto which we would have otherwise applied $v\in V$, to achieve the same effect).
  $$\sum\limits_{\lambda(n)}\prod\limits_{k\cdot l\in\lambda}\Big(\frac{\D_f\D_g^l(g(v))}{l!}\Big)^k\frac{n!}{k!}\Big(\Big(e^{\D_f}\Big)f(g(v))\Big)$$
  $$\implies$$
  \begin{equation}\label{eq:dCompositePoint}
  \sum\limits_{\lambda(n)}\prod\limits_{k\cdot l\in\lambda}\Big(\frac{\D_f\D_g^l(g(v))}{l!}\Big)^k\frac{n!}{k!}\Big(f(g(v))\Big)
  \end{equation}
 \end{opomba}
 The Theorem $\ref{izr:kompo}$ enables an invariant implementation of the operator of program composition in $\dP_n$, expressed as a tensor series through expressions $\eqref{eq:kompo}$ and $\eqref{eq:dComposite}$. 
 
 By fixing one mapping in $exp(\D_fe^{h\D_g}): \dP\times\dP\to\dP$, the operator $exp(\D_fe^{h\D_g})(g)$ performs a pullback of an arbitrary map through $g$. 
  \begin{equation}\label{eq:opKompo}
  exp(\D_fe^{h\D_g})(g): \dP\to\dP(g)
  \end{equation}
 Thus, through $\eqref{eq:kompo}$ and all its' descendants (exponents), the operator $(\ref{eq:opKompo})$ grants invariance to the point in execution of a program, which proves useful as invariants are at the center of proving algorithms' correctness. This is analogous to the principle of general covariance \cite{GeneralCovariance}[See section 7.1] in general relativity, the invariance of the form of physical laws under arbitrary differentiable coordinate transformations.
 
 With this we turn towards easing such calculations, towards completing them on the level of operators. The derivative $\frac{d}{dh}$ of $\eqref{eq:opKompo}$ is
 
 \begin{equation}\label{eq:dexp}
 \frac{d}{dh}exp(\D_fe^{h\D_g})(g)=\D_f(\D_gg)e^{h\D_g}exp(\D_fe^{h\D_g})(g)
 \end{equation}
 
 We note an important distinction to the operator $e^{h\D_g}$, the derivative of which is
 \begin{equation}\label{eq:de}
\frac{d}{dh}e^{h\D_g}=\D_ge^{h\D_g}
 \end{equation}
 We may now compute derivatives (of arbitrary order) of the pullback operator. As an example we compute the second derivative.
 $$(\frac{d}{dh})^2exp(\D_fe^{h\D_g})(g)=\frac{d}{dh}\Big(\D_f(\D_gg)e^{h\D_g}exp(\D_fe^{h\D_g})(g)\Big)$$
 which is by equations $\eqref{eq:dexp}$ and $\eqref{eq:de}$
 $$(\D_g\D_f(\D_gg))e^{h\D_g}exp(\D_fe^{h\D_g})(g)+(\D_f(\D_gg)\D_f(\D_gg))e^{2h\D_g}exp(\D_fe^{h\D_g})(g)$$
 using algebra and correct applications
 \begin{equation}\label{eq:d^2comp}
 (\D_f(\D^2_gg))e^{h\D_g}exp(\D_fe^{h\D_g})(g)+(\D^2_f(\D_gg)^2)e^{2h\D_g}exp(\D_fe^{h\D_g})(g)
 \end{equation}
 The operator is always shifted to the evaluating point $\eqref{eq:specProg}$ $v\in V$, thus, only the behavior in the limit as $h\to 0$ is of importance. Taking this limit in the expression $\eqref{eq:d^2comp}$ we obtain
 \begin{equation}
	(\D_f(\D^2_gg)+\D^2_f(\D_gg)^2)exp(\D_f)
 \end{equation}
 
 Thus, without imposing any additional rules, we computed the operator of the second derivative of composition with $g$, directly on the level of operators. The result of course matches the equation $\eqref{eq:dComposite}$ for $n=2$.
 
 As it is evident from the example, calculations using operators are far simpler, than direct manipulations of functional series, as it was done in the proof of Theorem $\ref{izr:kompo}$. This of course enables a simpler implementation, that functions over arbitrary function spaces. In the space that is spanned by $\{\D^n\}$ over $\dP$, derivatives of compositions may be expressed using only the product rule of Theorem $\ref{izr:prod}$, and $\eqref{eq:dexp}$ and $\eqref{eq:de}$, solely through the operators. Thus, explicit knowledge of rules for differentiating compositions is unnecessary, as it is contained in the structure of the operator $exp(\D_fe^{h\D_g})$ itself, which is differentiated using standard rules, as in the above example.
 
 \begin{equation}\label{eq:dkompo}
 \D^n(f\circ g)=\lim\limits_{\lVert h\rVert\to 0}(\frac{d}{dh})^nexp(\D_fe^{h\D_g})(g)(f)
 \end{equation}
 
 \begin{opomba}
 When applying $\D$ to an expression such as $(\D^n g)^k$, general rules of tensor calculus apply. 
% As the operators $\D^n$ are symmetric, this means
% \begin{equation}
% \D(\D g)^k=n(\D g)^{k-1}\D^2g
% \end{equation}
 \end{opomba}
 
   \begin{izrek}\label{izr:komp_homo}
   The operator $e^{h\D}$ commutes with composition over $\dP$
   \begin{equation}
   e^{h\D}(p_2\circ p_1)=e^{h\D}(p_2)\circ e^{h\D}(p_1)
   \end{equation}
   \end{izrek}
   
   \begin{proof}
   Follows from $\eqref{eq:pToPol}$ and Theorem $\ref{izr:kompo}$.
   \end{proof}
 
 It is useful to be able to use the $k$-th derivative of a program $p\in\dP$ as a working part of a different differentiable program. As such, we must be able to treat the derivative itself as a differentiable program $p^{\prime k}\in\dP$. 
 \begin{izrek}\label{izr:reductionMap}
 There exists a reduction of order map $\Phi:\dP_n\to\dP_{n-k}$, satisfying the relation
 \begin{equation}\label{eq:reductionMap}
 \forall_{p\in\dP}\exists_{p^{\prime k}\in\dP}\Big(\Phi\circ proj_{\{\D^k,...,\D^n\}}\big(e^\D_n(p)\big)=e^\D_{n-k}(p^{\prime k})\Big)
 \end{equation}
 with each component $\D^jp$ satisfying the relation $\eqref{eq:reductionMapSingle}$ of Theorem $\ref{izr:reductionMapSingle}$.
 \end{izrek}
 Thus, we gained the ability of writing differentiable programs using differentiable derivatives of other programs as part of their calculations, stressed as crucial (but lacking in most models) by other authors \cite{AD1}.
 
   \subsection{Functional transformation of programs}\label{sec:FTP}
   
   We motivate the following with an example. Lets suppose a hardware $H$, optimized for the set of functions $F=\{f_i:V\to V\}$. The set $F$ is specified by the manufacturer.
   
   With technological advances, switching the hardware is common, which can lead to a decline in performance. Thus, we would like to employ transformations of a program $P\in\dP$ in basis $F$. It is common to settle for a suboptimal algorithm, that is efficient on hardware $H$. Sub-optimality of the algorithm depends on the set $F$, whether it spans $P$, or not. A classic example of a transformation, is the Fourier transform in the basis $\{sin(nx), cos(mx)\}$, that spans $\dP$ (which as stated is not true for an arbitrary set $F$).
   
   By Theorem $\ref{izr:e^d}$, we may construct a map $\eqref{eq:pToPol}$ from the space of programs $\dP$, to the space of polynomials, using the operator $e^\D$. Thus, the image of $e^\D(P\in\dP)$ is integrable (with a suitable choice of domain). Using this, we may define an inner product over $\dP$.
   
  \begin{izrek}
  For $p_1,p_2\in\dP$ the inner product $\eta(p_1,p_2)$ is expressed as
  \begin{equation}
  	\eta(p_1,p_2)=\int_{\Omega}e^\D p_1(\vec{x})\cdot e^\D p_2(\vec{x})dx
  \end{equation}
  where $\cdot$ stands for the inner product on $V$, because $e^\D(p\in\dP):V\to V$.
  \end{izrek}
  
  \begin{proof}
  By Theorem $\ref{izr:prod}$, it holds that $e^\D(p_1\cdot p_2)=e^\D(p_1)\cdot e^\D(p_2)$. By linearity of the integral and a well defined inner product on $V$, the axioms of the inner product are satisfied.
  \end{proof}
  
  Thus, if $F$ is orthonormal, or has been made such with one of the established methods, $P$ can be expressed in the basis $F$ as
  
  \begin{equation}
 	P=\sum\limits_{\forall_{f_i\in F}}\frac{\eta(P,f_i)}{\eta(f_i,f_i)}f_i
  \end{equation}
  
  Seeking to avoid integration, using the developed tools, the problem is solvable using linear algebra. Let $e^\D_n$ denote the projection of the operator $e^\D$, onto the first $n$ basis vectors $\{\D^i\}$. We can, by Theorem $\ref{izr:e^d}$, construct a map $\eqref{eq:pToPol}$ from the space of programs, to the space of polynomials, with unknowns in $V^k$, using the operator $e^\D$. let $\X$ denote the basis of the space of polynomials, $\X=\{\prod\limits_{\forall_j} v_i^j\}$, where $v_i$ span $V$. Than, we can interpret $e^\D_n(P\in\dP)$ as a vector of linear combinations of $\X$, which is assumed heron.
  
  Thus, we define the tensor of basis transformation $F\to\X$
  
  \begin{equation}\label{eq:matTransF}
  T_{\X F}=\begin{pmatrix}
  e_n^\D(f_1) & e_n^\D(f_2) & \cdots & e_n^\D(f_n)
  \end{pmatrix}
  \end{equation}
  
  \begin{opomba}
  The tensor $\eqref{eq:matTransF}$ is a matrix, if the concerning mappings are $V^m\to V$, and tensor of rank $3$, if the concerning mappings are $V^m\to V^k$. Which can be interpreted as a sequence of matrices, one for each component of the image.
  \end{opomba}
  
  Thus, the tensor of basis transformation $\X\to F$ is
  
  \begin{equation}\label{eq:matTrans}
  T_{F\X}=T_{\X F}^{-1}
  \end{equation}
  
  Using the tensor $\eqref{eq:matTrans}$, we can easily perform basis transformations $\X\to F$. For a specific set $F$ (and consequentially a hardware $H$, upon which the set $F$ is conditioned), the tensor $\eqref{eq:matTrans}$ only has to be computed once, and can then be used for transforming arbitrary programs (while using the same operator $e^\D_n$).
  Thus, the coordinates of program $P\in\dP$ in basis $F$ are
  
  \begin{equation}\label{eq:P_F}
  	P_F=T_{F\X}\cdot e^\D(P)
  \end{equation}
  
  The expression $\eqref{eq:P_F}$ represents coordinates of program $P$ in basis $F$. Thus, the program is expressible as a linear combination of $f_i$, with components $P_F$ as coefficients.
  \begin{equation}
  P=\sum\limits_{i=0}^{n}{P_F}_if_i
  \end{equation}
  
  If $F$ does not span $\dP$, or we used the projection of the operator $e^\D_{n<N}$, the expression $P_F$ still represents the best possible approximation of the original program, on components $\{\D^n\}$, in basis $F$.
  
  It makes sense to, before computing the tensor $\eqref{eq:matTrans}$, to expand the set $F$, by mutual (nested) compositions, and gain mappings, that can not be expressed as linear combinations (but are still optimized for hardware $H$), and so increasing the power of the method.
  
 \subsection{Control structures}
 
 Until now, we restricted ourselves to operations, that change the memories' content. Along side assignment statements, we know control statements (ex. statements \texttt{if},
  \texttt{for}, \texttt{while}, ...). Control statements don't directly influence values of variables, but change the execution tree of the program. This of course affects the derivative. But, for a certain set of input variables, the execution of the program will always be the same. This is why we interpret control structures as a definition of a spline.
  
 Each control structure divides the space of parameters into different domains, in which the execution of the program is always the same. The entire program divides the space of all possible parameters to a finite set of domains $\{\Omega_i;\quad i=1,\ldots
  k\}$, where the programs' execution is always the same. As such, a program may in general be defined as a spline. For $\vec{x}\in\RR^n$
 \begin{equation}
   \label{eq:zlrprk_splosno}
   P(\vec{x}) =
   \begin{cases}
     P_{n_11}\circ P_{(n_1-1)1}\circ\ldots P_{11}(\vec{x});&\quad \vec{x}\in\Omega_1\\
     P_{n_22}\circ P_{(n_2-1)2}\circ\ldots P_{12}(\vec{x});&\quad \vec{x}\in\Omega_2\\
     \vdots&\quad\vdots\\
     P_{n_kk}\circ P_{(n_k-1)k}\circ\ldots P_{1k}(\vec{x});&\quad \vec{x}\in\Omega_k\\
   \end{cases}
 \end{equation}
 The operator $e^\D$ (at some point) of a program $P$, is of course dependent on initial parameters $\vec{x}$, and can also be expressed as a spline, inside domains $\Omega_i$
 \begin{equation}
   \label{eq:Dzlrprk_splosno}
   e^\D P_{\vec{x}} =
   \begin{cases}
     e^\D P_{n_11}\circ e^\D P_{(n_1-1)1}\circ\ldots\circ e^\D P_{11}(\vec{x});&\quad \vec{x}\in\interior(\Omega_1)\\
     e^\D P_{n_22}\circ e^\D P_{(n_2-1)2}\circ\ldots\circ e^\D P_{12}(\vec{x});&\quad \vec{x}\in\interior(\Omega_2)\\
     \vdots&\quad\vdots\\
     e^\D P_{n_kk}\circ e^\D P_{(n_k-1)k}\circ\ldots\circ e^\D P_{1k}(\vec{x});&\quad \vec{x}\in\interior(\Omega_k)\\
   \end{cases}
 \end{equation}
 The problem is on the edge of domains $\partial\Omega_i$, where the program $P$ may not be differentiable. A problem which may be avoided, with methods of the coming sections.
 
  \subsection{Iterators}
  
  Iterator is a functional, composing a program $p\in\dP$ with itself. Its' one of the most common tools in use, in most programming languages. For ease of expression, we denote the $n$-th iterate of a program $p\in\dP:V\to V$, as $p^n$. Thus, it is possible to view the iterate as a compositional exponent.
  
  Lets examine a simple example and familiarize ourselves with the concept. The program $p\in\dP$ is linearized around the point $p(a)=0$. Than for $v\in V$ we have $P(v)=\D p(a)\cdot v$. Consequentially the $n$-th iterate is $P^n(v)=(\D p(a))^n\cdot v$. Assuming the matrix $\D p(a)$ can be diagonalized $\D p(a)=S\Lambda S^{-1}$, where $\Lambda$ is a diagonal matrix of eigen values and $S$ the matrix of corresponding eigen vectors. Thus the $n$-th iterate is $P^n(v)=S\Lambda^nS^{-1}\cdot v$.
  
  Such an expression for the $n$-th iterate is a function of $n$, the number of iterations, and is as such differentiable with respect to exit conditions of the iterate. With this, we gained the ability to measure the change of the iterator, as dependent on continuous exit conditions. We seek to generalize the notion to general eigen-mappings.
  
  Let $\mathcal{I}_p$ be the cyclic monoid generated by $p:V\to V$ under composition $\circ$
  
  \begin{equation}
  \mathcal{I}_p=\{p^n:V\to V\iff p(a)=a\}
  \end{equation}
  
  Now, we may inquire towards changes in programs' iterations, as dependent on exit conditions. The answer would enable analysis and optimization of the number of iterations of the program. 
  
  Let $h$ be the map defined with the eigen equation \cite{CompoOper}
    
  \begin{equation}\label{eq:kh}
  h(p(x))=\lambda h(x)
  \end{equation}
   \begin{equation}
   h(a)=0
   \end{equation}
  
  We may conclude about the action of $h$ on $p$
  \begin{equation}
  h(p^n(x))=\lambda^nh(x)
  \end{equation}
  
  By differentiating $\eqref{eq:kh}$ at the fixed point, we get at the scalar $\lambda$.
  $$\D h(p(a))\D p(a)=\lambda\D h(a)$$
  $$\implies$$
  $$\D p(a)=\lambda$$
  On the domain $h(\mathcal{I})$ the maps $p^n$ become multiplication with $\lambda^n=(\D p(a))^n$. Obviously $\lambda^n=e^{\nu n}\iff \nu=ln(\lambda)$.
  
  We define iterating velocity as
  \begin{equation}
  v(p^n)=\D_np^n(x)
  \end{equation}
  Of course
  \begin{equation}
  v(p^n(a))=0
  \end{equation}
  which is deduced from the equation $\eqref{eq:kh}$ and coincides with intuition.
  
  With this we turn to computation of the iterating velocity
 
  $$\D_nh(p^n)=\D_n(e^{\nu n}h(x))$$
  $$\implies$$
  $$\D h(p^n(x))\D_np^n(x)=\nu e^{\nu n}h(x) \land e^{\nu n}h(x)=h(p^n(x))$$
  $$\implies$$
  \begin{equation}\label{eq:hitrostIteracije}
  v=\nu(\D^{-1}h)h
  \end{equation}
  
  For example, using methods of gradient descent, we use the velocity $\eqref{eq:hitrostIteracije}$, which gives the order and direction to the descent. Inductively, we could derive acceleration and higher order changes.
  
  Computation of the map $h$ and conditions for its' existence, for $p$ representable as a power series, was solved by Bridges \cite{BridgesShroeder}, derivation of which is omitted in this text, for the sake of brevity. By Theorem $\ref{izr:e^d}$, we may expand each program $p\in\dP$ by applying the operator $e^\D$ into a power series and employ said computation.
  
  As a special case of iterator usage, we examine accumulators, for which we derive an explicit form, as a function of exit conditions. Let $\Shift^n$ denote the operator, that performs a linear shift of a program $p$ in the direction $v$.
   
   \begin{equation}
   \Shift^n:\dP(v_0)\to \dP(v_0+nv)
   \end{equation}
    
    Let $\Shift$ be the cyclic group, for a specific $v_0, v\in V$ under composition and expanded by the addition operation.
    
   $$\Shift=\{\Shift^n\}$$   
   We define the $n$-th accumulator as 
   $$\mathcal{A}_n=(1+\Shift+\Shift^2+\cdots+\Shift^n)$$ Thus the expression is
   $$\sum\limits_{h=0}^{n}\dP(v_0+hv)=(1+\Shift+\Shift^2+\cdots+\Shift^n)(\dP)(v_0)$$
   As before, we are interested in changes as dependent on exit conditions. With this in mind, we explore the accumulator $\mathcal{A}_n$
   $$(1+\Shift+\Shift^2+\cdots+\Shift^n)=1+\Shift(1+\Shift+\Shift^2+\cdots+\Shift^{n-1})$$
   $$\implies$$
   $$1-\Shift^n=(1-S)\mathcal{A}_{n-1}$$
   $$\implies$$
   $$\mathcal{A}_{n-1}=(1-\Shift^n)(\frac{1}{1-\Shift})$$
   By Theorem $\ref{izr:e^d}$ we have
   $$e^{h\D}\dP(v_0)(v)=\dP(v_0+hv)\implies \Shift^h=e^{h\D}$$
   $$\implies$$
   $$\mathcal{A}_{n-1}=(1-\Shift^n)(\D^{-1})(\frac{\D}{1-e^{\D}})$$
   By Theorem $\ref{izr:linearnaNeodvisnost}$  there exists a space $\dP_n$, spanned by $\{\D^k\}$ over $V$. Thus the expression   
     
    \begin{equation}
    	\frac{h\D}{1-e^{h\D}}=\sum\limits_{n=0}^{\infty}c_n\frac{(h\D)^n}{n!}
    \end{equation}
    is well defined. It turns out that $c_i=B_i$, with $B_i$ being the $i$-th Bernoulli number. The operator $(1-\Shift^n)$ represents evaluation at endpoints (exit conditions).
    $$(1-\Shift^n)=\Bigg\vert_{v_0}^{nv}$$
    Thus $\mathcal{A}_{n-1}$ is expressed as
    $$\mathcal{A}_{n-1}=(\D^{-1})(\sum\limits_{i=0}^{\infty}c_i\frac{\D^i}{i!})\Bigg\vert_{v_0}^{nv}$$
    $$\implies$$
    \begin{equation}\label{eq:acumSum}
    \mathcal{A}_{n-1}=c_0\D^{-1}+\sum\limits_{i=1}^{\infty}c_i\frac{\D^{i-1}}{i!}\Bigg\vert_{v_0}^{nv}
    \end{equation}
    Explicit knowledge of only one inverse is needed. The need which dissipates, when one is interested in changes. Thus the accumulator  $\mathcal{A}_{n-1}$ can be expressed as
    \begin{equation}\label{eq:acumSumP}
    	\mathcal{A}_{n-1}\dP(v_0)=c_0\D^{-1}\dP(t)+\sum\limits_{i=1}^{\infty}c_i\frac{\D^{i-1}\dP(t)}{i!}\Bigg\vert_{v_0}^{nv}
    \end{equation}
    The expression is a function of exit conditions, so changes of arbitrary order are computable without expressions $\eqref{eq:kh}$. When $p:V\to V$, $\eqref{eq:acumSumP}$ is equivalent to the Euler-Maclaurin integral formula \cite{EulerSumAbramowitzStegun}.  We have an operator $\mathcal{A}_n\in\dP_n$, and we can compute with it alone, like in the case of composition $\eqref{eq:dkompo}$.

    $$\frac{d^k}{dn^k}\mathcal{A}_{n-1}=\frac{d^k}{dn^k}\Big((1-\Shift^n)(\frac{1}{1-e^\D})\Big)\land \Shift^n=e^{n\D}$$
        $$\implies$$
        $$\D^n e^{n\D}(\frac{1}{1-e^\D}) = \Shift^n(\frac{\D^n}{1-e^\D})$$
        $$\implies$$
        \begin{equation}
        \frac{d^k}{dn^k}\mathcal{A}_{n-1}P\vert_{n=N}(v)=\Big(\sum\limits_{i=0}^{\infty}c_i\frac{\D^{N-1+i}\dP(Nv)}{i!}\Big)\Big(v\Big)
        \end{equation}
        where evaluation at $v\in V$ performs the needed translation, as the image of the operator $\mathcal{A}_{n-1}$ is an element of the $V\otimes T(V^*)\simeq S(V^*)$.

\subsection{Transformations in practice} 

Branching of programs into domains $\eqref{eq:zlrprk_splosno}$ is done through conditional statements. The number of domain $\Omega_i$ equals cardinality of the set $\{\Omega_i\}$. Each conditional causes a branching in programs' execution tree.

\begin{izrek}\label{izr:st.zlepkov}
Cardinality of the set $\{\Omega_i\}$ equals $\lvert\{\Omega_i \}\rvert=2^k$, where $k$ is the number of conditionals contained within the program.
\end{izrek}
\begin{opomba}
Iterators, that do not change exit conditions within its' body, do not cause branching.
\end{opomba}

It follows from Theorem $\ref{izr:st.zlepkov}$, that the complexity of naive implementations of methods presented in Sections $\ref{sec:Vrsta}$ and $\ref{sec:FTP}$ are exponential (if we were to treat each domain $\Omega_i$ by itself). But, with correct application of Theorems developed in this paper, we may drastically reduce this down to linear. This is the subject of study in this section.
\vspace{10px}

\tikzstyle{decision} = [diamond, draw, fill=blue!20, 
    text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
    text width=5.5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']

\begin{figure}[!h]
\centering
\begin{tikzpicture}[node distance = 2cm, auto]
    \node [block] (p1) {$P_1$};
    \node [block,right of=p1,node distance=5cm] (ep1) {$e^\D(P_1)$};
    \node [block,right of=ep1,node distance=5cm] (tep1) {$T_{F\X}\cdot e^\D(P_1)$};
    
    \node [decision, below of=ep1,node distance=3cm] (v1) {\small{Branching}};
    
    \node [block,below of=v1,node distance=2.75cm] (ep2) {$e^\D(P_2)$};
    \node [block,left of=ep2,node distance=3.5cm] (p2) {$P_2$};
    \node [block,right of=ep2,node distance=3.5cm] (tep2) {$T_{F\X}\cdot e^\D(P_2)$};
    
    
    \node [block,below of=ep2,node distance=2.75cm] (ep3) {$e^\D(P_3)$};
    \node [block,left of=ep3,node distance=5cm] (p3) {$P_3$};
    \node [block,right of=ep3,node distance=5cm] (tep3) {$T_{F\X}\cdot e^\D(P_3)$};
    
    \path [line,dashed] (p1) -- node{$e^\D$}(ep1);
    \path [line,dashed] (p2) -- node{$e^\D$}(ep2);
    \path [line,dashed] (p3) -- node{$e^\D$}(ep3);
   
    \path [line,dashed] (ep1) -- node{$T_{F\X}$}(tep1);
    \path [line,dashed] (ep2) -- node{$T_{F\X}$}(tep2);
    \path [line,dashed] (ep3) -- node{$T_{F\X}$}(tep3);
		
	\coordinate [below of=ep2,node distance=1.5cm](C);
	\coordinate [right of=C,node distance=5.25cm](D);
	\coordinate [right of=v1,node distance=5.25cm](E);
	\coordinate [above of=E,node distance=1.5cm](F);
	\coordinate [above of=v1,node distance=1.5cm](G);
	\path[line](C)--(D)--(E)--(F)--(G);
	
	\coordinate [above of=ep2,node distance=1.5cm](C2);
	\coordinate [above of=p2,node distance=1.5cm](D2);
	\coordinate [above of=tep2,node distance=1.5cm](E2);
	\path[line](v1)--(ep2);
	\path[line] (C2)--(D2)--(p2);
	\path[line] (C2)--(E2)--(tep2);
	
	\coordinate [above of=ep3,node distance=1cm](D3);
	\coordinate [below of=ep2,node distance=1cm](C3);
	\coordinate [below of=p2,node distance=1cm](D3);
	\coordinate [below of=tep2,node distance=1cm](E3);
	\coordinate [below of=C,node distance=0.25cm](CC3);
	\draw (p2)--(D3)--(E3)--(tep2);
	\draw (ep2)--(C);
	
	\coordinate [below of=ep1,node distance=1cm](C4);
	\coordinate [below of=p1,node distance=1cm](D4);
	\coordinate [below of=tep1,node distance=1cm](E4);
	\draw (p1)--(D4)--(C4);
	\draw (tep1)--(E4)--(C4);
	\path[line](ep1)--(v1);
	
	\coordinate [above of=p3,node distance=1.25cm](C5);
	\coordinate [above of=ep3,node distance=1.25cm](D5);
	\coordinate [above of=tep3,node distance=1.25cm](E5);

	
	\coordinate [left of=p3,node distance=1.75cm](D2);
	\coordinate [left of=v1,node distance=6.75cm](F2);
	\coordinate [below of=D2,node distance=1.25cm](H2);
	\coordinate [below of=p3,node distance=1.25cm](I2);
	\coordinate [below of=ep3,node distance=1.25cm](J2);
	\coordinate [below of=tep3,node distance=1.25cm](K2);
	
	\path[line](v1)--(F2)--(D2)--(H2)--(I2)--(p3);
	\path[line](I2)--(J2)--(ep3);
	\path[line](J2)--(K2)--(tep3);
	
\end{tikzpicture}
\vspace{3px}
\caption{Transformation diagram} \label{fig:diagram} 
\end{figure}

\begin{izrek}
A program $P\in\dP$ can be equivalently represented with at most $2n+1$ applications of the operator $e^\D$, on $2n+1$ analytic programs.
\end{izrek}

\begin{proof}
	Source code of a program $P\in\dP$ can be represented by a directed graph, as shown in Figure $\ref{fig:diagram}$. Each branching causes a split in the execution tree, that after completion returns to the splitting point.
	By Theorem $\ref{izr:kompo}$, each of these branches can be viewed as a program $p_i$, for which it holds $$e^\D(p_n\circ p_{n-1}\circ\cdots\circ p_1)=e^\D(p_n)\circ e^\D(p_{n-1})\circ\cdots\circ e^\D(p_1)$$ by Theorem $\ref{izr:kompo}$.
	
	Thus, the source code contains $2n$ differentiable branches, from its' first branching on, not counting the branch leading up to it, upon which the application of the operator $e^\D$ is needed. Total of $2n+1$. By Theorem $\ref{izr:P_n}$, each of these branches is analytic.
\end{proof}

Images of the operator $e^\D$ and $T_{F\X}$  are elements of the original space $\dP$, which may be composed. Thus, for $P=p_3\circ p_2\circ p_1$, the following makes sense
\begin{equation}
P=\Big(p_3\circ e^ \D(p_2)\circ T_{F\X}e^\D(p_1)\Big) \in \dP
\end{equation}

The same holds true for all permutations of applications of operators $e^\D$, $T_{F\X}$ and $id$, as visible in Figure $\ref{fig:diagram}$.

\begin{opomba}
Equality depends on the set $F$, specified by the manufacturer of the hardware.
\end{opomba}

The transformation tensor $T_{F\X}$ is needed to be computed only once and can then on be applied to any program running on said hardware. The same holds true for each branch $p_i$, which can, by Theorem $\ref{izr:komp_homo}$, be freely composed amongst each other.

\begin{izrek}
Composition of images of the operator $e_n^\D(p_2)\circ e_n^\D(p_1)$ is computable in $\mathcal{O}(nlog^k(n))$, with $k$ being the dimension of $V$ and $n$ the number of basis vectors $\{\D^n\}$ upon which we are projecting.
\end{izrek}

\begin{proof}
	By $\eqref{eq:pToPol}$ images of the operator $e^\D$ are isomorphic to the symmetric algebra $S(V^*)$, composition of which is computable by FFT.
\end{proof}

Transformations $\eqref{eq:P_F}$ need to be computed only once. Complexity of the composition of images of transformations $T_{F\X}\cdot e^\D(p_2)\circ T_{F\X}\cdot e^\D(p_1)$ depends on complexities of $f_i\in F$.

Thus, we have acquired a tool, enabling us to choose the complexity of arbitrary algorithms $P\in\dP$, along with estimations and order of the error of the approximation. Simultaneously it enables transformations of the function basis of programs, and with it implementations, more inclined towards efficiency on a given hardware. After initial applications of operators and performed transformations the program may be used, as many times as pleased, towards the purpose it was initially designed for. By doing so, we may manipulate its' complexity (for $k\le n$) and switch between different code formulations ($p$, $e^\D(p)$, $T_{F\X}\cdot e^\D(p)$), as better suited to the nature of the domains' subset, upon which the program operates at a given moment.
  
\section{Analysis}
  
   The theory presented in this paper grants new insights through methods of analysis.  These methods are the object of study of this section.
  
  We will denote the fact, that some object $v$ has the property $X$, by $v\in X$. Suppose we have $v\notin X$, and desire a procedure that in some way modifies $v$ to have the property $X$. The desire is a procedure $P:v\notin X\to P(v)\in X$, that modifies $v$ in a some way. Let $S$ be some relation of similarity. We have
  \begin{equation}
  p\in P:v\notin X\to P(v)\in X\iff vSp(v)
  \end{equation}
  Usually such procedures are difficult to construct and may not even explicitly exist. An easier task is to construct a procedure $T$, whose output allows deduction of whether $v$ has the property $X$, or does not. We generalize such notions and develop methods of analyzing procedures $T$ that enable a construction of an algorithm
  \begin{equation}\label{eq:algA}
  A:T\to P
  \end{equation}
  
  Let $R$ be an equivalence relation 
  \begin{equation}\label{eq:equivRelation}
  v_1Rv_2\iff T(v_1)ST(v_2)
  \end{equation}
  where $S$ is some relation of similarity (ex. $\lvert1- T(v_1)\rvert\le c\land\lvert1- T(v_2)\rvert\le c$) and $M$ be the set of interest. Than $T$ partitions $M$ into equivalence classes, as by $R$ $\eqref{eq:equivRelation}$. The above property $X$, may be viewed as one of these equivalence classes $X_i$.
  
  Let $T=T_n\circ T_{n-1}\circ\cdots\circ T_1$, for simplicity, but it could just as easily be a spline as in $\eqref{eq:zlrprk_splosno}$ (by a specific $T_i$ containing a control structure). Here $T_i=\varepsilon_k\circ\ldots \varepsilon_1$, is a conceptual step in the procedure. Then, we can inquire about and measure the importance $K$ of an arbitrary $T_i$, to a partitioning into a specific equivalence class $X_j$. $\eqref{eq:equivRelation}$. Thus, we are able to extract $T_{X_j}$, the set of steps of importance, to such a partitioning. We treat $T_i\circ\cdots T_1$ as the input of the procedure $T^n_i=T_n\circ\cdots\circ T_{i+1}$.
  
  We present a simple algorithm towards such a goal, which can easily be enhanced with a more sophisticated choice of the measure of importance, as allowed by the theory. Here, the measure is simply the norm of the differential, as it measures the rate of change in the relation.
  
  \begin{opomba}
  For generality of the method, we express the computation of a differential as $\D_{T_k}T^n_k$, knowing than in $\dP$, it would have been computed as $e^{\D_{T_k}}_1(T^n_k)\in V\oplus V\otimes V^*$, and projecting the result onto $V\otimes V^*$.
  \end{opomba}
  
  
\begin{algorithm}[h]
\caption{Steps of importance to a partitioning}
\label{alg:partitioning}
\begin{algorithmic}[1]
\Procedure{Steps of importance to a partitioning}{}
\For{each $X_i$}
\For{each $v\in X_i$}
\For{each $T^n_k$}
\State add norm $\D_{T_k}T^n_k$ to $N_{T_k}$
\EndFor
\EndFor
\State threshold $T_t\in T_{X_i}\iff N_{T_t}\ge c$  
\State insert $T_{X_i}$ in $I$
\EndFor
\State return $I$
\EndProcedure
\end{algorithmic}
\end{algorithm}
Thus we get $I=\{T_{X_i}\}$, where $T_{X_i}$ is the set of steps $T_j$ measured as important by Algorithm $\ref{alg:partitioning}$. Each set $T_{X_j}$ contains conceptual steps, portions of the source code, whose action on the input contributes most in a partitioning to the equivalence class $X_j$. As such, the set $I$ shows to be useful in studying properties of the set $M$, as it reveals to what action, a property of an element $v\in M$ reacts to, as we probe it using the method. The perturbations reveal procedures internal structure and input-archetypes. 

As shown by Algorithm $\ref{alg:partitioning}$, we arrived at sub-procedures $T_i$, to which properties $X_j$ react to most strongly. Thus, using the algebra and calculus of algorithms of this paper, we can construct a procedure that changes an arbitrary $v\in M$, in such a way that maximizes the action of $T_i\in T_{X_j}$, for the desired property $X_j$. With this we turn towards constructing algorithm $A$ $\eqref{eq:algA}$.

We extract a once differentiable derivative of $T^n_j$ by applying $e^{\D_{T_j}}_2$ and projecting the image onto $\{\D,\D^2\}$,
\begin{equation}\label{eq:T'}
T^\prime_j=\Phi\circ proj_{\{\D,\D^2\}}e^{\D_{T_j}}_2(T^n_j)
\end{equation}
as, its' derivative (second derivative of $T$) is needed for further compositions $\eqref{eq:kompo}$ by Theorem $\ref{izr:kompo}$, where $\Phi$ is the reduction of order map $\eqref{eq:reductionMap}$ of Theorem $\ref{izr:reductionMap}$. As such, it holds $T^\prime\in\dP_1$. We do so, for each $T_j\in T_{X_i}$ contained in $I$, where $X_i$ is the desired property.

Let $C\in\dP$ be a convex map, some measure of magnitude. Thus, we arrive at the expression to be maximized
\begin{equation}
E_j=C\circ T^\prime_j
\end{equation}
\begin{algorithm}[h]
\caption{Appoint property $X_j$ to $v\in M$}
\label{alg:appoint}
\begin{algorithmic}[1]
\Procedure{Appoint property $X_j$ to $v\in M$}{}
\State initialize path $\gamma$ with $v$
\For{each step}
\For{each $T_i\in T_{X_j}$}
\State extract $T^\prime=\Phi\circ proj_{\{\D,\D^2\}}e^{\D_{T_i}}_2(T^n_j)$ $\eqref{eq:T'}$
\State compute the energy $E=C\circ T^\prime$
\State extract the derivative $\D_v E=proj_{\{\D\}}(e^{\D_v}_1(E))$
\State add $\D_v E$ to $\mathcal{D}$
\State insert $v$ to $\gamma$
\EndFor
\State update v by $step(\mathcal{D},v)$
\EndFor
\State return $\gamma$
\EndProcedure
\end{algorithmic}
\end{algorithm}
The derivative with respect to $v$ is extracted by applying $e^{\D_{v}}_1$ and projecting the image onto $\D$,
\begin{equation}
\D_vE_j=proj_{\{\D\}}(e^{\D_v}_1E_j)
\end{equation}
as it needs not to be differentiable for this purpose.

For simplicity, assume $step$ to represent one optimization step, using a first derivative in its' execution. A method easily enhanced by any of more sophisticated optimization techniques readily available. 

As shown by Algorithm $\ref{alg:appoint}$, we arrived at a path $\gamma$, whose end point is the transformed $v\in M$, now endowed with the property $X_j$. Thus, using the theory, with the procedure $T$ as an input variable in Algorithm $\ref{alg:appoint}$, we constructed algorithm $A: T\to P$ $\eqref{eq:algA}$. When $v\in M$ represents an object upon which we wish to impose the property $X_j$, we may do so using the derived procedure $A(T)=P$
\begin{equation}\label{eq:procedureP}
P:v\notin X_j\to v\in X_j
\end{equation}

The derived procedures $P$ $\eqref{eq:procedureP}$, are a generalization of special cases such as Google's Deep Dream project \cite{DeepDream}, where the program represents a neural network, with neurons with high activity filling the set of important conceptual steps $T_{X_j}$ and the resulting $v\in X_j$ is an altered image.

\begin{izrek}[Path to change]
Path $\gamma$ is the path to change of least resistance.
\end{izrek}

Path to change of least resistance is the flow curve outputted by Algorithm $\ref{alg:appoint}$. When $T$ serves as a simulation, with $v$ modeling a real-life object of study, the procedure opens new doors in analyzing real-life phenomena. Through it, we may observe how $v$ evolves through iterations and study different stages of change, which serves as a useful insight when designing procedures causing change in real-life phenomena.

  \printbibliography
\end{document}

